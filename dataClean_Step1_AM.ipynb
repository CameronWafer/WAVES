{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3294cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "258328d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>type</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_day</th>\n",
       "      <th>start_year</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>session</th>\n",
       "      <th>do</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DO2</td>\n",
       "      <td>L</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>18:44:45</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>2:02:15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>16:43:57</td>\n",
       "      <td>18:45:00</td>\n",
       "      <td>2:01:03</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>13:17:10</td>\n",
       "      <td>15:17:32</td>\n",
       "      <td>2:00:22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_a</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>8:00:27</td>\n",
       "      <td>8:52:32</td>\n",
       "      <td>0:52:05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_b</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>8:56:13</td>\n",
       "      <td>10:12:29</td>\n",
       "      <td>1:16:16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    obs type  start_month  start_day  start_year start_time stop_time  \\\n",
       "0   1    DO2    L           10          3        2017   18:44:45  20:47:00   \n",
       "1   1    DO1    H           10          6        2017   16:43:57  18:45:00   \n",
       "2   2    DO1    H            7         24        2017   13:17:10  15:17:32   \n",
       "3   2  DO2_a    A            7         25        2017    8:00:27   8:52:32   \n",
       "4   2  DO2_b    A            7         25        2017    8:56:13  10:12:29   \n",
       "\n",
       "  duration  session  do  \n",
       "0  2:02:15        1   1  \n",
       "1  2:01:03        2   2  \n",
       "2  2:00:22        1   1  \n",
       "3  0:52:05        2   2  \n",
       "4  1:16:16        3   3  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AM import (both log and behavior)\n",
    "behav_am_df = pd.read_excel(\"C:/Users/HELIOS-300/Desktop/Data/am_behposture_onesheet.xlsx\", engine='openpyxl')\n",
    "log_df = pd.read_csv(\"C:/Users/HELIOS-300/Desktop/Data/DO_LOG_final.csv\", encoding='utf-8')\n",
    "\n",
    "behav_am_df = behav_am_df[behav_am_df[\"Event_Type\"] == \"State start\"]\n",
    "\n",
    "# Convert timedelta columns back to time strings (HH:MM:SS format)\n",
    "# This fixes the \"0 days 00:00:00\" display issue\n",
    "for col in behav_am_df.columns:\n",
    "    if pd.api.types.is_timedelta64_dtype(behav_am_df[col]):\n",
    "        # Convert timedelta to time string by adding to a base date and extracting time\n",
    "        base_date = pd.Timestamp('1900-01-01')\n",
    "        behav_am_df[col] = (base_date + behav_am_df[col]).dt.strftime('%H:%M:%S')\n",
    "\n",
    "# special log AM setup / cleanup\n",
    "# Extract ID from id column (e.g., \"AM02\" -> 2)\n",
    "log_df['id'] = log_df[\"id\"].astype(str)\n",
    "log_df['id'] = log_df['id'].str.extract(r'AM(\\d{2})', expand=False)\n",
    "log_df['id'] = pd.to_numeric(log_df[\"id\"], errors=\"coerce\").astype(np.int64)\n",
    "\n",
    "log_df[\"do\"] = log_df[\"session\"]\n",
    "\n",
    "# special behav AM setup / cleanup\n",
    "# Extract ID from Observation column (e.g., \"AM02DO1_J_FINAL_R\" -> 2)\n",
    "behav_am_df['id'] = behav_am_df['Observation'].str.extract(r'AM(\\d{2})', expand=False).astype(np.int64)\n",
    "# Extract DO from Observation column (e.g., \"AM02DO1_J_FINAL_R\" -> 1)\n",
    "behav_am_df['do'] = behav_am_df['Observation'].str.extract(r'DO(\\d+)', expand=False).astype(np.int64)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f2ce9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>type</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_day</th>\n",
       "      <th>start_year</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>session</th>\n",
       "      <th>do</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DO2</td>\n",
       "      <td>L</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>18:44:45</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>2:02:15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>16:43:57</td>\n",
       "      <td>18:45:00</td>\n",
       "      <td>2:01:03</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>13:17:10</td>\n",
       "      <td>15:17:32</td>\n",
       "      <td>2:00:22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_a</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>8:00:27</td>\n",
       "      <td>8:52:32</td>\n",
       "      <td>0:52:05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_b</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>8:56:13</td>\n",
       "      <td>10:12:29</td>\n",
       "      <td>1:16:16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    obs type  start_month  start_day  start_year start_time stop_time  \\\n",
       "0   1    DO2    L           10          3        2017   18:44:45  20:47:00   \n",
       "1   1    DO1    H           10          6        2017   16:43:57  18:45:00   \n",
       "2   2    DO1    H            7         24        2017   13:17:10  15:17:32   \n",
       "3   2  DO2_a    A            7         25        2017    8:00:27   8:52:32   \n",
       "4   2  DO2_b    A            7         25        2017    8:56:13  10:12:29   \n",
       "\n",
       "  duration  session  do  \n",
       "0  2:02:15        1   1  \n",
       "1  2:01:03        2   2  \n",
       "2  2:00:22        1   1  \n",
       "3  0:52:05        2   2  \n",
       "4  1:16:16        3   3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad82c2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity events: 731, Posture events: 9678, Other: 103\n",
      "Activity expanded: 996552, Posture expanded: 857325\n",
      "Merged result: 1184000 rows\n",
      "After encoding, behav_am_df_7 shape: (1177432, 23)\n",
      "activity_type NaN: 81524\n",
      "posture_wbm NaN: 516\n",
      "Stabilization: activity_type 81524 -> 0, posture_wbm 516 -> 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>date_time</th>\n",
       "      <th>rel_time</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>posture_waves</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:44:45 PM</td>\n",
       "      <td>10/3/2017 06:44:45 PM</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:44:45 PM</td>\n",
       "      <td>10/3/2017 06:44:45 PM</td>\n",
       "      <td>00:00:01</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:44:45 PM</td>\n",
       "      <td>10/3/2017 06:44:45 PM</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:44:45 PM</td>\n",
       "      <td>10/3/2017 06:44:45 PM</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:44:45 PM</td>\n",
       "      <td>10/3/2017 06:44:45 PM</td>\n",
       "      <td>00:00:04</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  obs       date         time              date_time  rel_time  \\\n",
       "0  1.0  1.0  10/3/2017  06:44:45 PM  10/3/2017 06:44:45 PM  00:00:00   \n",
       "1  1.0  1.0  10/3/2017  06:44:45 PM  10/3/2017 06:44:45 PM  00:00:01   \n",
       "2  1.0  1.0  10/3/2017  06:44:45 PM  10/3/2017 06:44:45 PM  00:00:02   \n",
       "3  1.0  1.0  10/3/2017  06:44:45 PM  10/3/2017 06:44:45 PM  00:00:03   \n",
       "4  1.0  1.0  10/3/2017  06:44:45 PM  10/3/2017 06:44:45 PM  00:00:04   \n",
       "\n",
       "  activity_type posture_waves intensity  \n",
       "0    les_social    mixed_move     light  \n",
       "1    les_social    mixed_move     light  \n",
       "2    les_social    mixed_move     light  \n",
       "3    les_social    mixed_move     light  \n",
       "4    les_social    mixed_move     light  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Behavior AM data cleaning\n",
    "# Separates activity and posture tracks to preserve simultaneous events at same timestamp\n",
    "\n",
    "# quick log data cleaning\n",
    "log_df['date'] = pd.to_datetime({\n",
    "    'year': pd.to_numeric(log_df['start_year'], errors='coerce'),\n",
    "    'month': pd.to_numeric(log_df['start_month'], errors='coerce'),\n",
    "    'day': pd.to_numeric(log_df['start_day'], errors='coerce'),\n",
    "}, errors='coerce').dt.strftime('%#m/%#d/%Y')\n",
    "\n",
    "log_df.drop(columns=[\"start_month\", \"start_day\", \"start_year\"], inplace=True)\n",
    "log_df2 = log_df.loc[:, [\"id\", \"do\", \"date\", \"start_time\"]].copy()\n",
    "\n",
    "# Convert log_df2 start_time to 24-hour HH:MM:SS\n",
    "s = log_df2['start_time'].astype(str).str.strip()\n",
    "\n",
    "# Support both with and without seconds\n",
    "_dt1 = pd.to_datetime(s, format='%I:%M:%S %p', errors='coerce')\n",
    "_dt2 = pd.to_datetime(s, format='%I:%M %p', errors='coerce')\n",
    "\n",
    "log_df2.loc[:, 'start_time'] = _dt1.fillna(_dt2).dt.strftime('%H:%M:%S')\n",
    "log_df2.loc[:, 'date_time'] = log_df2['date'].astype(str).str.strip() + ' ' + log_df2['start_time'].astype(str).str.strip()\n",
    "\n",
    "log_df2.rename(columns={\"start_time\" : \"time\", \"do\" : \"obs\"}, inplace=True)\n",
    "log_df2 = log_df2.drop(columns=[\"time\", \"date_time\"])\n",
    "\n",
    "\n",
    "# Start behavior cleaning\n",
    "# a) why does unnamed 17 and 18 exist? no one knows :)\n",
    "behav_am_df1 = behav_am_df.drop(columns=[\"Date_Time_Absolute_dmy_hmsf\", \n",
    "\"Date_dmy\", \n",
    "\"Time_Absolute_hms\", \n",
    "\"Time_Absolute_f\",\n",
    "\"Event_Log\"])\n",
    "\n",
    "# add \"id\" and \"do\" style ID's from LOG into ACT behavior file so we can join\n",
    "def add_id_do_split(df, source_col='Observation', id_col='id', do_col='do', inplace=True):\n",
    "    parts = df[source_col].str.split('_', expand=True)\n",
    "    id_series = pd.to_numeric(parts[1], errors='coerce').astype('Int64')\n",
    "    do_series = pd.to_numeric(parts[2], errors='coerce').astype('Int64')\n",
    "    if inplace:\n",
    "        df[id_col] = id_series\n",
    "        df[do_col] = do_series\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out[id_col] = id_series\n",
    "    out[do_col] = do_series\n",
    "    return out\n",
    "\n",
    "# add a column from LOG onto Behavior based on \"id\" and \"do\"\n",
    "def add_col_from_other_df_merge(\n",
    "    left: pd.DataFrame,\n",
    "    right: pd.DataFrame,\n",
    "    left_keys: list,\n",
    "    right_keys: list,\n",
    "    right_value_col: str,\n",
    "    new_col_name: str | None = None,\n",
    "    how: str = 'left',\n",
    "    validate: str = 'many_to_one'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a single column from `right` to `left` by joining on two (or more) key columns.\n",
    "    \"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = right_value_col\n",
    "\n",
    "    right_subset = right[right_keys + [right_value_col]].rename(\n",
    "        columns={right_value_col: new_col_name}\n",
    "    )\n",
    "    merged = left.merge(\n",
    "        right_subset,\n",
    "        how=how,\n",
    "        left_on=left_keys,\n",
    "        right_on=right_keys,\n",
    "        validate=validate\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "# Merge with log_df on 'id' and 'do' to get 'start_time'\n",
    "# Note: 'do' is already extracted from Observation column in cell 1, so we merge on both keys\n",
    "behav_am_df2 = behav_am_df1.copy()\n",
    "\n",
    "# Ensure 'do' column exists before merge (safety check)\n",
    "if 'do' not in behav_am_df2.columns:\n",
    "    behav_am_df2['do'] = behav_am_df2['Observation'].str.extract(r'DO(\\d+)', expand=False).astype(np.int64)\n",
    "\n",
    "behav_am_df3 = behav_am_df2.merge(\n",
    "    log_df[['id', 'session', 'start_time', 'date']].rename(columns={'session': 'do'}),\n",
    "    on=['id', 'do'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "series_temp = behav_am_df3.pop(\"start_time\")\n",
    "behav_am_df3.insert(0, \"start_time\", series_temp)\n",
    "\n",
    "behav_am_df4 = behav_am_df3.drop(index=behav_am_df3.index[behav_am_df3[\"Event_Type\"] != \"State start\"])\n",
    "\n",
    "# parse start_time (supports both 12-hour \"8:20:19 AM\"/\"8:20 AM\" and 24-hour \"18:44:45\"/\"18:44\")\n",
    "s = behav_am_df4['start_time'].astype(str).str.strip()\n",
    "# Try 12-hour formats first\n",
    "dt1 = pd.to_datetime(s, format='%I:%M:%S %p', errors='coerce')\n",
    "dt2 = pd.to_datetime(s, format='%I:%M %p', errors='coerce')\n",
    "# Try 24-hour formats\n",
    "dt3 = pd.to_datetime(s, format='%H:%M:%S', errors='coerce')\n",
    "dt4 = pd.to_datetime(s, format='%H:%M', errors='coerce')\n",
    "# Combine: try 12-hour first, then 24-hour\n",
    "behav_am_df4['start_time_dt'] = dt1.fillna(dt2).fillna(dt3).fillna(dt4)\n",
    "\n",
    "# parse Time_Relative_hmsf (supports \"HH:MM:SS(.f)\", \"MM:SS(.f)\", \"SS(.f)\")\n",
    "r = behav_am_df4['Time_Relative_hmsf'].astype(str).str.strip()\n",
    "r = r.str.replace(',', '.', regex=False).str.replace(';', '.', regex=False)\n",
    "\n",
    "td = pd.Series(pd.NaT, index=r.index, dtype='timedelta64[ns]')\n",
    "mask_hms = r.str.count(':') == 2\n",
    "mask_ms  = r.str.count(':') == 1\n",
    "mask_sec = r.str.fullmatch(r'\\d+(\\.\\d+)?')\n",
    "mask_blank = r.eq('') | r.str.lower().isin(['nan', 'none'])\n",
    "\n",
    "td.loc[mask_hms] = pd.to_timedelta(r[mask_hms], errors='coerce')\n",
    "td.loc[mask_ms]  = pd.to_timedelta('00:' + r[mask_ms], errors='coerce')  # prefix hours\n",
    "td.loc[mask_sec] = pd.to_timedelta(r[mask_sec].astype(float), unit='s')\n",
    "td.loc[mask_blank] = pd.NaT\n",
    "\n",
    "behav_am_df4['time_relative_td'] = td\n",
    "\n",
    "# sum to produce the new start time\n",
    "behav_am_df5 = behav_am_df4.copy()\n",
    "behav_am_df5['start_time_new'] = behav_am_df5['start_time_dt'] + behav_am_df5['time_relative_td']\n",
    "\n",
    "# time-only display strings (no date)\n",
    "behav_am_df5['start_time_str'] = behav_am_df5['start_time_dt'].dt.strftime('%I:%M:%S %p')\n",
    "behav_am_df5['start_time_new_str'] = behav_am_df5['start_time_new'].dt.strftime('%I:%M:%S %p')\n",
    "\n",
    "# drop intermediates, rename, and position between the first two columns\n",
    "drop_cols = [c for c in ['start_time_dt','time_relative_td','start_time_new','start_time_str'] if c in behav_am_df5.columns]\n",
    "behav_am_df5 = behav_am_df5.drop(columns=drop_cols)\n",
    "\n",
    "behav_am_df5 = behav_am_df5.rename(columns={'start_time_new_str': 'start_time_new'})\n",
    "\n",
    "first_cols = ['start_time', 'start_time_new', 'Time_Relative_hmsf']\n",
    "other_cols = [c for c in behav_am_df5.columns if c not in first_cols]\n",
    "behav_am_df5 = behav_am_df5[first_cols + other_cols]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# NEW APPROACH: Separate activity and posture tracks before expansion\n",
    "# ============================================================================\n",
    "\n",
    "# --- Helper functions (shared) ---\n",
    "def _parse_hms_to_seconds(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(',', '.', regex=False).str.replace(';', '.', regex=False)\n",
    "\n",
    "    td = pd.Series(pd.NaT, index=s.index, dtype='timedelta64[ns]')\n",
    "    mask_hms = s.str.count(':') == 2           # H:M:S(.f)\n",
    "    mask_ms  = s.str.count(':') == 1           # M:S(.f)\n",
    "    mask_sec = s.str.fullmatch(r'\\d+(\\.\\d+)?') # seconds only\n",
    "    mask_blank = s.eq('') | r.str.lower().isin(['nan', 'none'])\n",
    "\n",
    "    td.loc[mask_hms] = pd.to_timedelta(s[mask_hms], errors='coerce')\n",
    "    td.loc[mask_ms]  = pd.to_timedelta('00:' + s[mask_ms], errors='coerce')\n",
    "    if mask_sec.any():\n",
    "        td.loc[mask_sec] = pd.to_timedelta(s[mask_sec].astype(float), unit='s')\n",
    "    td.loc[mask_blank] = pd.NaT\n",
    "\n",
    "    return td.dt.total_seconds()\n",
    "\n",
    "def _format_hms(seconds_float: float, decimals: int = 0) -> str:\n",
    "    if pd.isna(seconds_float):\n",
    "        return np.nan\n",
    "    scale = 10 ** decimals\n",
    "    total_units = int(round(seconds_float * scale))\n",
    "    secs = total_units // scale\n",
    "    frac_units = total_units % scale\n",
    "    h = secs // 3600\n",
    "    m = (secs % 3600) // 60\n",
    "    s = secs % 60\n",
    "    if decimals == 0:\n",
    "        return f'{h:02d}:{m:02d}:{s:02d}'\n",
    "    return f'{h:02d}:{m:02d}:{s:02d}.{frac_units:0{decimals}d}'\n",
    "\n",
    "# Normalization for classification\n",
    "_def_ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def _normalize_behavior(value: object) -> str | None:\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    s = str(value).strip().lower()\n",
    "    s = s.replace('–', '-').replace('—', '-')\n",
    "    s = _def_ws_re.sub(' ', s)\n",
    "    return s\n",
    "\n",
    "# Classify behavior as domain activity or posture\n",
    "_domain_prefixes = {\n",
    "    'sl-', 'pc-', 'ha-', 'ca-', 'wrk-', 'edu-', 'org-', 'pur-', 'eat-', 'les-', 'ex-', 'trav-', 'other-'\n",
    "}\n",
    "_posture_prefixes = {\n",
    "    'sb-', 'la-', 'wa-', 'sp-'\n",
    "}\n",
    "\n",
    "def _classify_behavior(behavior_val):\n",
    "    \"\"\"Returns 'activity', 'posture', or 'other'\"\"\"\n",
    "    norm = _normalize_behavior(behavior_val)\n",
    "    if not norm:\n",
    "        return 'other'\n",
    "    # Check domain prefixes\n",
    "    for prefix in _domain_prefixes:\n",
    "        if norm.startswith(prefix):\n",
    "            return 'activity'\n",
    "    # Check posture prefixes\n",
    "    for prefix in _posture_prefixes:\n",
    "        if norm.startswith(prefix):\n",
    "            return 'posture'\n",
    "    # Handle special cases\n",
    "    if norm in {'private/not coded', 'start posture', 'start behavior'}:\n",
    "        return 'other'\n",
    "    return 'other'\n",
    "\n",
    "df = behav_am_df5.copy()\n",
    "df['_seconds'] = _parse_hms_to_seconds(df['Time_Relative_hms'])\n",
    "df = df.sort_values(['Observation', '_seconds'], kind='mergesort')\n",
    "\n",
    "# Classify each row\n",
    "df['_track'] = df['Behavior'].apply(_classify_behavior)\n",
    "\n",
    "# Split into activity and posture dataframes\n",
    "activity_df = df[df['_track'] == 'activity'].copy()\n",
    "posture_df = df[df['_track'] == 'posture'].copy()\n",
    "\n",
    "print(f\"Activity events: {len(activity_df)}, Posture events: {len(posture_df)}, Other: {(df['_track'] == 'other').sum()}\")\n",
    "\n",
    "# --- Expand activity track to per-second ---\n",
    "def expand_track_to_seconds(track_df, track_name='track'):\n",
    "    \"\"\"Expand a track (activity or posture) to per-second resolution\"\"\"\n",
    "    out_groups = []\n",
    "    \n",
    "    for obs_value, g in track_df.groupby('Observation', sort=False):\n",
    "        g = g.copy()\n",
    "        g = g[~g['_seconds'].isna()]\n",
    "        if g.empty:\n",
    "            continue\n",
    "        \n",
    "        g['_event_second'] = np.floor(g['_seconds']).astype(int)\n",
    "        \n",
    "        # Keep last within each second\n",
    "        g_last = (\n",
    "            g.sort_values(['_event_second', '_seconds'], kind='mergesort')\n",
    "             .drop_duplicates(subset=['_event_second'], keep='last')\n",
    "        )\n",
    "        \n",
    "        min_s = float(g['_seconds'].min())\n",
    "        max_s = float(g['_seconds'].max())\n",
    "        \n",
    "        if np.isclose(min_s, 0.0):\n",
    "            start_second = 0\n",
    "            flag_first = False\n",
    "        else:\n",
    "            start_second = int(np.ceil(min_s))\n",
    "            flag_first = True\n",
    "        \n",
    "        end_second = int(np.floor(max_s))\n",
    "        if end_second < start_second:\n",
    "            end_second = start_second\n",
    "        \n",
    "        seconds_grid = np.arange(start_second, end_second + 1, dtype=int)\n",
    "        \n",
    "        base = g_last.set_index('_event_second').sort_index()\n",
    "        first_index_second = int(np.floor(min_s))\n",
    "        full_index = np.arange(first_index_second, end_second + 1, dtype=int)\n",
    "        aligned = base.reindex(full_index).ffill()\n",
    "        \n",
    "        take = aligned.loc[seconds_grid].copy()\n",
    "        take.reset_index(drop=False, inplace=True)\n",
    "        take.rename(columns={'_event_second': '_second'}, inplace=True)\n",
    "        \n",
    "        # Time strings\n",
    "        time_strings = [_format_hms(s, decimals=0) for s in seconds_grid]\n",
    "        if flag_first and len(time_strings) > 0:\n",
    "            flagged = min_s + 0.01\n",
    "            time_strings[0] = _format_hms(flagged, decimals=2)\n",
    "        \n",
    "        take['Time_Relative_hms_new'] = time_strings\n",
    "        \n",
    "        out_groups.append(take)\n",
    "    \n",
    "    if not out_groups:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result = pd.concat(out_groups, axis=0, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "# Expand both tracks\n",
    "activity_expanded = expand_track_to_seconds(activity_df, 'activity')\n",
    "posture_expanded = expand_track_to_seconds(posture_df, 'posture')\n",
    "\n",
    "print(f\"Activity expanded: {len(activity_expanded)}, Posture expanded: {len(posture_expanded)}\")\n",
    "\n",
    "# --- Merge activity and posture on (Observation, second) ---\n",
    "# Keep columns needed from each track (including modifiers from BOTH)\n",
    "activity_cols_keep = ['Observation', '_second', 'Time_Relative_hms_new', 'Behavior', 'Modifier_1', 'Modifier_2', 'Modifier_3', \n",
    "                      'start_time_new', 'id', 'do']\n",
    "posture_cols_keep = ['Observation', '_second', 'Behavior', 'Modifier_2']  # Modifier_2 for intensity\n",
    "\n",
    "activity_subset = activity_expanded[activity_cols_keep].rename(columns={'Behavior': 'Behavior_activity', \n",
    "                                                                          'Modifier_1': 'Modifier_1_activity',\n",
    "                                                                          'Modifier_2': 'Modifier_2_activity',\n",
    "                                                                          'Modifier_3': 'Modifier_3'})\n",
    "posture_subset = posture_expanded[posture_cols_keep].rename(columns={'Behavior': 'Behavior_posture',\n",
    "                                                                       'Modifier_2': 'Modifier_2_posture'})\n",
    "\n",
    "# Full outer merge to get all seconds from both tracks\n",
    "merged = activity_subset.merge(\n",
    "    posture_subset,\n",
    "    on=['Observation', '_second'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_posture')\n",
    ")\n",
    "\n",
    "# Fill observation metadata forward\n",
    "merged = merged.sort_values(['Observation', '_second'], kind='mergesort')\n",
    "for col in ['id', 'do', 'Time_Relative_hms_new', 'start_time_new']:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged.groupby('Observation')[col].ffill().bfill()\n",
    "\n",
    "# Combine Behaviors: use activity behavior for encoding activity_type, posture behavior for encoding posture\n",
    "merged['Behavior'] = merged['Behavior_activity'].fillna(merged['Behavior_posture'])\n",
    "\n",
    "# Combine Modifier_1 and Modifier_3 (activity-related modifiers)\n",
    "merged['Modifier_1'] = merged['Modifier_1_activity']\n",
    "merged['Modifier_3'] = merged['Modifier_3']\n",
    "\n",
    "# Combine Modifier_2 (intensity): prefer posture track, fallback to activity track\n",
    "merged['Modifier_2'] = merged['Modifier_2_posture'].fillna(merged['Modifier_2_activity'])\n",
    "\n",
    "# Rename _second to rel_time for final output\n",
    "merged['rel_time'] = merged['Time_Relative_hms_new']\n",
    "\n",
    "behav_am_df_6 = merged.copy()\n",
    "\n",
    "# Cleanup only intermediate helper columns, but KEEP Behavior_activity and Behavior_posture for encoding!\n",
    "for c in ['_seconds', '_second', '_track', 'Modifier_1_activity', 'Modifier_2_activity', 'Modifier_2_posture']:\n",
    "    if c in behav_am_df_6.columns:\n",
    "        behav_am_df_6 = behav_am_df_6.drop(columns=c)\n",
    "\n",
    "print(f\"Merged result: {len(behav_am_df_6)} rows\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODING: Activity and Posture (independent tracks, same as before)\n",
    "# ============================================================================\n",
    "\n",
    "behav_am_df_7 = behav_am_df_6.copy()\n",
    "\n",
    "# Mapping from canonical Activity_Type to (activity_type, broad_domain, waves_domain)\n",
    "_activity_type_to_meta = {\n",
    "    'SL- Sleep': ('sleep', 'sleep', 'household_personal'),\n",
    "    'PC- Groom, Health-Related': ('pc_groom', 'personal', 'household_personal'),\n",
    "    'PC- Other Personal Care': ('pc_other', 'personal', 'household_personal'),\n",
    "    'HA- Housework': ('ha_housework', 'household', 'household_personal'),\n",
    "    'HA- Food Prep and Cleanup': ('ha_food', 'household', 'household_personal'),\n",
    "    'HA- Interior Maintenance, Repair, & Decoration': ('ha_interior', 'maintenance_repair', 'household_personal'),\n",
    "    'HA- Exterior Maintenance, Repair, & Decoration': ('ha_exterior', 'maintenance_repair', 'household_personal'),\n",
    "    'HA- Lawn, Garden and Houseplants': ('ha_lawn', 'lawn_garden', 'household_personal'),\n",
    "    'HA- Animals and Pets': ('ha_pets', 'household', 'household_personal'),\n",
    "    'HA- Household Management/Other household activities': ('ha_other', 'household', 'household_personal'),\n",
    "    'CA- Caring for and Helping Children': ('care_children', 'household', 'household_personal'),\n",
    "    'CA- Caring for and Helping Adults': ('care_adults', 'household', 'household_personal'),\n",
    "    'WRK- General**': ('work_general', 'work_education', 'work_education'),\n",
    "    'WRK- Desk/Screen Based': ('work_screen', 'work_education', 'work_education'),\n",
    "    'EDU- Taking Class, Research, Homework': ('edu_class', 'work_education', 'work_education'),\n",
    "    'EDU- Extracurricular': ('edu_other', 'work_education', 'work_education'),\n",
    "    'ORG- Church, Spiritual': ('com_church', 'purchase_other', 'purchase_other'),\n",
    "    'Volunteer Work (ORG - Volunteer Work)': ('com_volunteer', 'purchase_other', 'purchase_other'),\n",
    "    'PUR- Purchasing Goods and Services': ('com_purchase', 'purchase_other', 'purchase_other'),\n",
    "    'EAT- Eating and Drinking, Waiting': ('ha_eat', 'personal', 'household_personal'),\n",
    "    'LES- Socializing, Communicating, Non-Screen Based': ('les_social', 'leisure', 'leisure'),\n",
    "    'LES- Screen-Based (TV, Video Game, Computer, Phone)': ('les_screen', 'Leisure_Screen', 'leisure'),\n",
    "    'EX- Participating in Sport, Exercise or Recreation***': ('ex_sport', 'exercise', 'leisure'),\n",
    "    'EX- Attending Sport, Exercise Recreation Event, or Performance': ('les_attend', 'leisure', 'leisure'),\n",
    "    'TRAV- Passenger (Car/Truck/Motorcycle)': ('trav_pass', 'Trav_car', 'transportation'),\n",
    "    'TRAV- Driver (Car/Truck/Motorcycle)': ('trav_drive', 'Trav_car', 'transportation'),\n",
    "    'TRAV- Passenger (Bus, Train, Tram, Plane, Boat, Ship)': ('trav_pass', 'Trav_public', 'transportation'),\n",
    "    'TRAV- Biking': ('trav_bike', 'active_transportation', 'transportation'),\n",
    "    'TRAV-Walking': ('trav_walk', 'active_transportation', 'transportation'),\n",
    "    'TRAV- General': ('trav_other', 'transportation', 'transportation'),\n",
    "    'OTHER- Non-Codable (delete these rows from dataset)': ('non_codable', 'non_codable', 'non_codable'),\n",
    "}\n",
    "\n",
    "# Map raw Behavior values to canonical Activity_Type keys above\n",
    "_alias_to_activity_type = {\n",
    "    'sl- sleep': 'SL- Sleep',\n",
    "    'pc- groom, health-related': 'PC- Groom, Health-Related',\n",
    "    'pc- other personal care': 'PC- Other Personal Care',\n",
    "    'ha- housework': 'HA- Housework',\n",
    "    'ha- food prep and cleanup': 'HA- Food Prep and Cleanup',\n",
    "    'ha- interior maintenance, repair, & decoration': 'HA- Interior Maintenance, Repair, & Decoration',\n",
    "    'ha- exterior maintenance, repair, & decoration': 'HA- Exterior Maintenance, Repair, & Decoration',\n",
    "    'ha- lawn, garden and houseplants': 'HA- Lawn, Garden and Houseplants',\n",
    "    'ha- animals and pets': 'HA- Animals and Pets',\n",
    "    'ha- household management/other household activities': 'HA- Household Management/Other household activities',\n",
    "    'ca- caring for and helping children': 'CA- Caring for and Helping Children',\n",
    "    'ca- caring for and helping adults': 'CA- Caring for and Helping Adults',\n",
    "    'wrk- general': 'WRK- General**',\n",
    "    'wrk- screen based': 'WRK- Desk/Screen Based',\n",
    "    'edu- taking class, research, homework': 'EDU- Taking Class, Research, Homework',\n",
    "    'edu- extracurricular': 'EDU- Extracurricular',\n",
    "    'org- church, spiritual': 'ORG- Church, Spiritual',\n",
    "    'org- volunteer': 'Volunteer Work (ORG - Volunteer Work)',\n",
    "    'pur- purchasing goods and services': 'PUR- Purchasing Goods and Services',\n",
    "    'eat- eating and drinking, waiting': 'EAT- Eating and Drinking, Waiting',\n",
    "    'les- socializing, communicating, leisure time not screen': 'LES- Socializing, Communicating, Non-Screen Based',\n",
    "    'les- screen based leisure time (tv, video game, computer)': 'LES- Screen-Based (TV, Video Game, Computer, Phone)',\n",
    "    'les- screen-based (tv, video game, computer, phone)': 'LES- Screen-Based (TV, Video Game, Computer, Phone)',\n",
    "    'ex- participating in sport, exercise or recreation': 'EX- Participating in Sport, Exercise or Recreation***',\n",
    "    'ex- attending sport, recreational event, or performance': 'EX- Attending Sport, Exercise Recreation Event, or Performance',\n",
    "    'trav- passenger (car/truck/motorcycle)': 'TRAV- Passenger (Car/Truck/Motorcycle)',\n",
    "    'trav- driver (car/truck/motorcycle)': 'TRAV- Driver (Car/Truck/Motorcycle)',\n",
    "    'trav- passenger (bus, train, tram, plane, boat, ship)': 'TRAV- Passenger (Bus, Train, Tram, Plane, Boat, Ship)',\n",
    "    'trav- biking': 'TRAV- Biking',\n",
    "    'trav- walking': 'TRAV-Walking',\n",
    "    'trav-walking': 'TRAV-Walking',\n",
    "    'trav- general': 'TRAV- General',\n",
    "    'other- non codable': 'OTHER- Non-Codable (delete these rows from dataset)',\n",
    "    'private/not coded': 'OTHER- Non-Codable (delete these rows from dataset)',\n",
    "}\n",
    "\n",
    "def _map_behavior_to_activity_type(value: object) -> str | None:\n",
    "    s = _normalize_behavior(value)\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.startswith('les- screen'):\n",
    "        return 'LES- Screen-Based (TV, Video Game, Computer, Phone)'\n",
    "    if s.startswith('trav- passenger (bus'):\n",
    "        return 'TRAV- Passenger (Bus, Train, Tram, Plane, Boat, Ship)'\n",
    "    return _alias_to_activity_type.get(s)\n",
    "\n",
    "# Build Activity_Type from Behavior_activity column (preserved from activity track)\n",
    "if 'Behavior_activity' in behav_am_df_7.columns:\n",
    "    behav_am_df_7['Activity_Type'] = behav_am_df_7['Behavior_activity'].apply(_map_behavior_to_activity_type)\n",
    "else:\n",
    "    # Fallback: classify on the fly from merged Behavior\n",
    "    behav_am_df_7['Activity_Type'] = behav_am_df_7['Behavior'].apply(\n",
    "        lambda b: _map_behavior_to_activity_type(b) if _classify_behavior(b) == 'activity' else None\n",
    "    )\n",
    "\n",
    "# EX modifier handling\n",
    "if 'Modifier_1' in behav_am_df_7.columns:\n",
    "    mask_ex = behav_am_df_7['Activity_Type'] == 'EX- Participating in Sport, Exercise or Recreation***'\n",
    "    mask_m1 = behav_am_df_7['Modifier_1'].notna()\n",
    "    mask_apply = mask_ex & mask_m1\n",
    "    if mask_apply.any():\n",
    "        mod1_norm = (\n",
    "            behav_am_df_7.loc[mask_apply, 'Modifier_1']\n",
    "            .astype(str).str.strip().str.lower()\n",
    "            .str.replace(r'\\s+', '-', regex=True).str.replace('/', '-')\n",
    "        )\n",
    "        behav_am_df_7.loc[mask_apply, 'Activity_Type'] = 'EX-' + mod1_norm\n",
    "\n",
    "# work_type from Modifier_3\n",
    "work_labels = {'WRK- General**', 'WRK- Desk/Screen Based'}\n",
    "if 'Modifier_3' in behav_am_df_7.columns:\n",
    "    def _mk_work_type(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        raw = str(x).strip()\n",
    "        raw = re.sub(r'^\\s*sp-\\s*', '', raw, flags=re.IGNORECASE)\n",
    "        s = re.sub(r\"\\s+\", '_', raw.lower()).replace('/', '_')\n",
    "        s = s.replace('hospiltality', 'hospitality')\n",
    "        return f\"work_{s}\" if s else np.nan\n",
    "    behav_am_df_7['work_type_raw'] = behav_am_df_7['Modifier_3'].apply(_mk_work_type)\n",
    "else:\n",
    "    behav_am_df_7['work_type_raw'] = np.nan\n",
    "\n",
    "# Expand Activity_Type to three encoded columns\n",
    "cols = ['activity_type', 'broad_domain', 'waves_domain']\n",
    "\n",
    "def _activity_meta_lookup(activity_type: object):\n",
    "    if isinstance(activity_type, str) and activity_type.startswith('EX-'):\n",
    "        return ('ex_sport', 'exercise', 'leisure')\n",
    "    return _activity_type_to_meta.get(activity_type)\n",
    "\n",
    "behav_am_df_7[cols] = behav_am_df_7['Activity_Type'].map(_activity_meta_lookup).apply(\n",
    "    lambda tpl: pd.Series(tpl if isinstance(tpl, tuple) else (np.nan, np.nan, np.nan))\n",
    ")\n",
    "\n",
    "# Detect grouping\n",
    "if 'Observation' in behav_am_df_7.columns:\n",
    "    _group_cols = ['Observation']\n",
    "elif {'id','do'}.issubset(behav_am_df_7.columns):\n",
    "    _group_cols = ['id','do']\n",
    "else:\n",
    "    _group_cols = None\n",
    "\n",
    "# Forward-fill Activity_Type within observation\n",
    "if _group_cols is not None:\n",
    "    behav_am_df_7['Activity_Type'] = behav_am_df_7.groupby(_group_cols)['Activity_Type'].ffill()\n",
    "    behav_am_df_7[cols] = behav_am_df_7['Activity_Type'].map(_activity_meta_lookup).apply(\n",
    "        lambda tpl: pd.Series(tpl if isinstance(tpl, tuple) else (np.nan, np.nan, np.nan))\n",
    "    )\n",
    "\n",
    "# Posture encoding\n",
    "def _map_posture_wbm_from_behavior(value: object) -> str | None:\n",
    "    s = _normalize_behavior(value)\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.startswith('sb-sitting'):\n",
    "        return 'sitting'\n",
    "    if s.startswith('sb-lying') or s.startswith('sb- lying'):\n",
    "        return 'lying'\n",
    "    if s.startswith('la- kneeling'):\n",
    "        return 'kneel_squat'\n",
    "    if s == 'la- stretching':\n",
    "        return 'stretch'\n",
    "    if s == 'la- stand and move':\n",
    "        return 'stand_move'\n",
    "    if s == 'la- stand':\n",
    "        return 'stand'\n",
    "    if s in {'wa- walk', 'wa- walking', 'trav- walking', 'trav-walking'}:\n",
    "        return 'walk'\n",
    "    if s in {'wa-walk with load', 'wa- walk with load'}:\n",
    "        return 'walk_load'\n",
    "    if s == 'wa- ascend stairs':\n",
    "        return 'ascend'\n",
    "    if s == 'wa- descend stairs':\n",
    "        return 'descend'\n",
    "    if s == 'wa- running':\n",
    "        return 'running'\n",
    "    if s == 'sp- bike':\n",
    "        return 'biking'\n",
    "    if s in {'sp- other sport movement', 'sp- swing', 'sp -kick', 'sp- jump'}:\n",
    "        return 'sport_move'\n",
    "    if s == 'sp- muscle strengthening':\n",
    "        return 'muscle_strength'\n",
    "    if s == 'private/not coded':\n",
    "        return 'not_coded'\n",
    "    return None\n",
    "\n",
    "_posture_meta = {\n",
    "    'sitting': ('sedentary', 'sedentary'),\n",
    "    'lying': ('sedentary', 'sedentary'),\n",
    "    'kneel_squat': ('sedentary', 'mixed_move'),\n",
    "    'stretch': ('sport', 'sport'),\n",
    "    'stand': ('stand_move', 'mixed_move'),\n",
    "    'stand_move': ('stand_move', 'mixed_move'),\n",
    "    'walk': ('walk', 'walk'),\n",
    "    'walk_load': ('mod_walk', 'walk'),\n",
    "    'ascend': ('mod_walk', 'walk'),\n",
    "    'descend': ('mod_walk', 'walk'),\n",
    "    'running': ('running', 'running'),\n",
    "    'biking': ('biking', 'biking'),\n",
    "    'sport_move': ('sport', 'sport'),\n",
    "    'muscle_strength': ('sport', 'sport'),\n",
    "    'not_coded': ('not_coded', 'not_coded'),\n",
    "}\n",
    "\n",
    "# Build posture from Behavior_posture column (preserved from posture track)\n",
    "# CRITICAL: must use Behavior_posture, not merged Behavior, to avoid losing posture when both activity and posture exist at same second\n",
    "if 'Behavior_posture' in behav_am_df_7.columns:\n",
    "    behav_am_df_7['posture_wbm'] = behav_am_df_7['Behavior_posture'].apply(_map_posture_wbm_from_behavior)\n",
    "else:\n",
    "    # fallback: try to extract from merged Behavior (but this will miss simultaneous events)\n",
    "    behav_am_df_7['posture_wbm'] = behav_am_df_7['Behavior'].apply(\n",
    "        lambda b: _map_posture_wbm_from_behavior(b) if _classify_behavior(b) == 'posture' else None\n",
    "    )\n",
    "\n",
    "_broad_waves = behav_am_df_7['posture_wbm'].map(lambda k: _posture_meta.get(k, (np.nan, np.nan)))\n",
    "behav_am_df_7[['posture_broad', 'posture_waves']] = pd.DataFrame(_broad_waves.tolist(), index=behav_am_df_7.index)\n",
    "\n",
    "# Forward-fill posture within observation\n",
    "if _group_cols is not None:\n",
    "    for _c in ['posture_wbm', 'posture_broad', 'posture_waves']:\n",
    "        behav_am_df_7[_c] = behav_am_df_7.groupby(_group_cols)[_c].ffill()\n",
    "\n",
    "# waves_sedentary\n",
    "def _waves_sed_vec(posture_wbm, activity_type):\n",
    "    \"\"\"Vectorized waves_sedentary computation\"\"\"\n",
    "    result = pd.Series(index=posture_wbm.index, dtype='object')\n",
    "    \n",
    "    mask_sit = posture_wbm == 'sitting'\n",
    "    mask_drive = activity_type.isin({'trav_drive', 'trav_pass'})\n",
    "    result.loc[mask_sit & mask_drive] = 'sed_drive'\n",
    "    result.loc[mask_sit & ~mask_drive] = 'sedentary'\n",
    "    \n",
    "    mask_lying_kneel = posture_wbm.isin({'lying', 'kneel_squat'})\n",
    "    result.loc[mask_lying_kneel] = 'sedentary'\n",
    "    \n",
    "    mask_active = posture_wbm.notna() & ~mask_sit & ~mask_lying_kneel\n",
    "    result.loc[mask_active] = 'active'\n",
    "    \n",
    "    return result\n",
    "\n",
    "behav_am_df_7['waves_sedentary'] = _waves_sed_vec(behav_am_df_7['posture_wbm'], behav_am_df_7['activity_type'])\n",
    "\n",
    "# Intensity encoding\n",
    "# intensity typically comes from posture events (sb-, la-, wa-, sp-) so use Behavior_posture first\n",
    "def _posture_intensity(value: object) -> str | None:\n",
    "    s = _normalize_behavior(value)\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.startswith('sb-sitting') or s.startswith('sb-lying') or s.startswith('sb- lying') or s.startswith('la- kneeling'):\n",
    "        return 'sedentary'\n",
    "    if s in {'la- stand', 'la- stand and move', 'la- stretching'}:\n",
    "        return 'light'\n",
    "    return None\n",
    "\n",
    "# try posture behavior first, then fall back to merged behavior\n",
    "if 'Behavior_posture' in behav_am_df_7.columns:\n",
    "    behav_am_df_7['intensity'] = behav_am_df_7['Behavior_posture'].apply(_posture_intensity)\n",
    "    # fill from activity behavior where posture didn't provide intensity\n",
    "    _mask_missing = behav_am_df_7['intensity'].isna()\n",
    "    behav_am_df_7.loc[_mask_missing, 'intensity'] = behav_am_df_7.loc[_mask_missing, 'Behavior_activity'].apply(_posture_intensity)\n",
    "else:\n",
    "    behav_am_df_7['intensity'] = behav_am_df_7['Behavior'].apply(_posture_intensity)\n",
    "\n",
    "# Fill from Modifier_2 only where intensity is still missing\n",
    "if 'Modifier_2' in behav_am_df_7.columns:\n",
    "    def _norm_intensity(m) -> str | None:\n",
    "        if pd.isna(m):\n",
    "            return None\n",
    "        s = str(m).strip().lower()\n",
    "        if not s:\n",
    "            return None\n",
    "        if s.startswith('vig'):\n",
    "            return 'vigorous'\n",
    "        if s.startswith('mod'):\n",
    "            return 'moderate'\n",
    "        if s == 'light':\n",
    "            return 'light'\n",
    "        if s == 'sedentary':\n",
    "            return 'sedentary'\n",
    "        return None\n",
    "    _mask_missing = behav_am_df_7['intensity'].isna()\n",
    "    behav_am_df_7.loc[_mask_missing, 'intensity'] = behav_am_df_7.loc[_mask_missing, 'Modifier_2'].apply(_norm_intensity)\n",
    "\n",
    "# Forward-fill intensity within observation\n",
    "if _group_cols is not None:\n",
    "    behav_am_df_7['intensity'] = behav_am_df_7.groupby(_group_cols)['intensity'].ffill()\n",
    "\n",
    "# waves_intensity\n",
    "behav_am_df_7['waves_intensity'] = behav_am_df_7['intensity'].map(lambda x: 'mvpa' if x in {'moderate', 'vigorous'} else x)\n",
    "\n",
    "# Finalize work_type\n",
    "if 'work_type_raw' in behav_am_df_7.columns:\n",
    "    if _group_cols is not None:\n",
    "        behav_am_df_7['work_type_raw'] = behav_am_df_7.groupby(_group_cols)['work_type_raw'].ffill()\n",
    "    behav_am_df_7['work_type'] = np.where(\n",
    "        behav_am_df_7['Activity_Type'].isin(work_labels),\n",
    "        behav_am_df_7['work_type_raw'],\n",
    "        np.nan,\n",
    "    )\n",
    "    behav_am_df_7 = behav_am_df_7.drop(columns=['work_type_raw'])\n",
    "\n",
    "# Drop non-codable\n",
    "_non_codable_mask = (\n",
    "    behav_am_df_7['Activity_Type'] == 'OTHER- Non-Codable (delete these rows from dataset)'\n",
    ") | (\n",
    "    behav_am_df_7['Behavior'].astype(str).str.strip().str.lower().isin(['private/not coded'])\n",
    ")\n",
    "behav_am_df_7 = behav_am_df_7.loc[~_non_codable_mask].copy()\n",
    "\n",
    "print(f\"After encoding, behav_am_df_7 shape: {behav_am_df_7.shape}\")\n",
    "print(f\"activity_type NaN: {behav_am_df_7['activity_type'].isna().sum()}\")\n",
    "print(f\"posture_wbm NaN: {behav_am_df_7['posture_wbm'].isna().sum()}\")\n",
    "\n",
    "# Stabilize both tracks with ffill+bfill\n",
    "if _group_cols is not None:\n",
    "    # Activity track\n",
    "    _before_act = behav_am_df_7['Activity_Type'].isna().sum()\n",
    "    ff_act = behav_am_df_7.groupby(_group_cols, sort=False)['Activity_Type'].ffill()\n",
    "    bf_act = behav_am_df_7.groupby(_group_cols, sort=False)['Activity_Type'].bfill()\n",
    "    behav_am_df_7['Activity_Type'] = ff_act.fillna(bf_act)\n",
    "    \n",
    "    # Recompute activity meta\n",
    "    behav_am_df_7[cols] = behav_am_df_7['Activity_Type'].map(_activity_meta_lookup).apply(\n",
    "        lambda tpl: pd.Series(tpl if isinstance(tpl, tuple) else (np.nan, np.nan, np.nan))\n",
    "    )\n",
    "    _after_act = behav_am_df_7['Activity_Type'].isna().sum()\n",
    "    \n",
    "    # Posture track\n",
    "    _before_pos = behav_am_df_7['posture_wbm'].isna().sum()\n",
    "    ff_pos = behav_am_df_7.groupby(_group_cols, sort=False)['posture_wbm'].ffill()\n",
    "    bf_pos = behav_am_df_7.groupby(_group_cols, sort=False)['posture_wbm'].bfill()\n",
    "    behav_am_df_7['posture_wbm'] = ff_pos.fillna(bf_pos)\n",
    "    \n",
    "    # Recompute posture meta\n",
    "    _pw = behav_am_df_7['posture_wbm'].map(lambda k: _posture_meta.get(k, (np.nan, np.nan)))\n",
    "    behav_am_df_7[['posture_broad', 'posture_waves']] = pd.DataFrame(_pw.tolist(), index=behav_am_df_7.index)\n",
    "    _after_pos = behav_am_df_7['posture_wbm'].isna().sum()\n",
    "    \n",
    "    # Recompute waves_sedentary\n",
    "    behav_am_df_7['waves_sedentary'] = _waves_sed_vec(behav_am_df_7['posture_wbm'], behav_am_df_7['activity_type'])\n",
    "    \n",
    "    print(f\"Stabilization: activity_type {_before_act} -> {_after_act}, posture_wbm {_before_pos} -> {_after_pos}\")\n",
    "\n",
    "# Final cleanup: drop Behavior_activity and Behavior_posture now that encoding is complete\n",
    "for c in ['Behavior_activity', 'Behavior_posture']:\n",
    "    if c in behav_am_df_7.columns:\n",
    "        behav_am_df_7 = behav_am_df_7.drop(columns=c)\n",
    "        \n",
    "# Build behav_copy for final output\n",
    "# First, merge 'date' from log_df since it may have been lost during track merging\n",
    "behav_am_df_7_with_date = behav_am_df_7.merge(\n",
    "    log_df[['id', 'do', 'date']],\n",
    "    on=['id', 'do'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "behav_copy = behav_am_df_7_with_date[[\"id\", \"do\", \"Time_Relative_hms_new\", \"activity_type\", \"posture_waves\", \"intensity\", \"start_time_new\", \"date\"]].copy()\n",
    "behav_copy = behav_copy.rename(columns={\"do\": \"obs\", \"Time_Relative_hms_new\": \"rel_time\", \"start_time_new\": \"time\"})\n",
    "\n",
    "# Create date_time column: date + \" \" + time\n",
    "# Convert time to string if it's datetime, then combine with date\n",
    "if pd.api.types.is_datetime64_any_dtype(behav_copy['time']):\n",
    "    behav_copy['time'] = behav_copy['time'].dt.strftime('%I:%M:%S %p')\n",
    "behav_copy['date_time'] = behav_copy['date'].astype(str) + \" \" + behav_copy['time'].astype(str)\n",
    "\n",
    "# Reorder columns to exact specification: id, obs, date, time, date_time, rel_time, activity_type, posture_waves, intensity\n",
    "behav_copy = behav_copy[[\"id\", \"obs\", \"date\", \"time\", \"date_time\", \"rel_time\", \"activity_type\", \"posture_waves\", \"intensity\"]]\n",
    "\n",
    "behav_copy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eeae9492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>date_time</th>\n",
       "      <th>rel_time</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>posture_waves</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/3/2017</td>\n",
       "      <td>06:47:26 PM</td>\n",
       "      <td>10/3/2017 06:47:26 PM</td>\n",
       "      <td>00:02:41</td>\n",
       "      <td>les_screen</td>\n",
       "      <td>sedentary</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  obs       date         time              date_time  rel_time  \\\n",
       "1000  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1001  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1002  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1003  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1004  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1005  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1006  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1007  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1008  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "1009  1.0  1.0  10/3/2017  06:47:26 PM  10/3/2017 06:47:26 PM  00:02:41   \n",
       "\n",
       "     activity_type posture_waves  intensity  \n",
       "1000    les_screen     sedentary  sedentary  \n",
       "1001    les_screen     sedentary  sedentary  \n",
       "1002    les_screen     sedentary  sedentary  \n",
       "1003    les_screen     sedentary  sedentary  \n",
       "1004    les_screen     sedentary  sedentary  \n",
       "1005    les_screen     sedentary  sedentary  \n",
       "1006    les_screen     sedentary  sedentary  \n",
       "1007    les_screen     sedentary  sedentary  \n",
       "1008    les_screen     sedentary  sedentary  \n",
       "1009    les_screen     sedentary  sedentary  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_copy.iloc[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a2d62d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE DATA VALIDATION TESTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST 1: Original Data Sources Integrity\n",
      "================================================================================\n",
      "✓ PASS: Original behav_am_df loaded\n",
      "  → Rows: 10,512, Columns: 20, IDs: 26, Observations: 61\n",
      "✓ PASS: All required columns present in behav_am_df\n",
      "✓ PASS: Original log_df loaded\n",
      "  → Rows: 57, IDs: 26, DO values: 4\n",
      "✓ PASS: All required columns present in log_df\n",
      "✓ PASS: No duplicate (id, do) combinations in log_df\n",
      "\n",
      "================================================================================\n",
      "TEST 2: ID and DO Extraction from Observation\n",
      "================================================================================\n",
      "✓ PASS: ID extraction: No nulls, 26 unique IDs\n",
      "✓ PASS: DO extraction: No nulls, 2 unique DO values\n",
      "✓ PASS: Observation pattern extraction verified\n",
      "\n",
      "================================================================================\n",
      "TEST 3: Merge with log_df\n",
      "================================================================================\n",
      "✗ FAIL: Merge: 337 rows (3.21%) missing start_time\n",
      "✓ PASS: ID preserved after merge: No nulls\n",
      "✓ PASS: DO preserved after merge: No nulls\n",
      "⚠ WARN: Some (id, do) combinations in merge not in log_df: 2 unmatched\n",
      "\n",
      "================================================================================\n",
      "TEST 4: start_time_new Computation\n",
      "================================================================================\n",
      "⚠ WARN: start_time_new: 337 rows (3.21%) are null\n",
      "✓ PASS: start_time_new format is correct (HH:MM:SS AM/PM)\n",
      "\n",
      "================================================================================\n",
      "TEST 5: Track Separation and Expansion\n",
      "================================================================================\n",
      "✓ PASS: Tracks separated: Activity=731, Posture=9,678\n",
      "⚠ WARN: 103 rows classified as 'other'\n",
      "✓ PASS: Tracks expanded: Activity=996,552, Posture=857,325\n",
      "✓ PASS: activity_expanded has all required columns\n",
      "✓ PASS: activity_expanded: No null IDs\n",
      "✓ PASS: activity_expanded: No null DO values\n",
      "✓ PASS: posture_expanded has all required columns\n",
      "✓ PASS: posture_expanded: No null IDs\n",
      "✓ PASS: posture_expanded: No null DO values\n",
      "\n",
      "================================================================================\n",
      "TEST 6: Merged Activity and Posture Tracks\n",
      "================================================================================\n",
      "✓ PASS: Merged result: 1,184,000 rows\n",
      "✓ PASS: Observation: No nulls\n",
      "✓ PASS: id: No nulls\n",
      "✓ PASS: do: No nulls\n",
      "✓ PASS: Time_Relative_hms_new: No nulls\n",
      "✓ PASS: start_time_new in merged: No nulls\n",
      "✓ PASS: Each Observation has consistent id and do values\n",
      "\n",
      "================================================================================\n",
      "TEST 7: Final Output (behav_copy)\n",
      "================================================================================\n",
      "✓ PASS: Final output shape: 1,177,432 rows, 9 columns\n",
      "✗ FAIL: Missing required columns: ['start_time_new']\n",
      "✓ PASS: id: No nulls\n",
      "✓ PASS: obs: No nulls\n",
      "✓ PASS: rel_time: No nulls\n",
      "✓ PASS: activity_type: No nulls\n",
      "✓ PASS: posture_waves: No nulls\n",
      "⚠ WARN: intensity: 743 nulls (0.06%) - within acceptable range\n",
      "✗ FAIL: start_time_new: Column missing\n",
      "✓ PASS: id column has numeric type\n",
      "✓ PASS: obs column has numeric type\n",
      "⚠ WARN: 191210 duplicate rows found\n",
      "✓ PASS: id range: 1 to 27, obs range: 1 to 2\n",
      "✓ PASS: All id values match log_df\n",
      "\n",
      "================================================================================\n",
      "TEST 8: Data Loss Check\n",
      "================================================================================\n",
      "✓ PASS: No observation loss: 61 observations preserved\n",
      "✓ PASS: All original observations present in final output\n",
      "\n",
      "================================================================================\n",
      "TEST 9: Data Consistency Checks\n",
      "================================================================================\n",
      "✓ PASS: rel_time format is correct (HH:MM:SS)\n",
      "✓ PASS: All activity_type values are valid\n",
      "✓ PASS: All posture_waves values are valid\n",
      "✓ PASS: All intensity values are valid\n",
      "\n",
      "================================================================================\n",
      "TEST 10: Summary Statistics\n",
      "================================================================================\n",
      "\n",
      "Final Output Summary:\n",
      "  Total rows: 1,177,432\n",
      "  Total columns: 9\n",
      "  Unique IDs: 26\n",
      "  Unique observations: 2\n",
      "  Unique activity types: 22\n",
      "  Activity type distribution:\n",
      "    work_general: 282,162 (24.0%)\n",
      "    edu_class: 167,907 (14.3%)\n",
      "    les_screen: 149,715 (12.7%)\n",
      "    work_screen: 132,512 (11.3%)\n",
      "    ha_food: 123,769 (10.5%)\n",
      "    les_social: 93,327 (7.9%)\n",
      "    ha_eat: 89,187 (7.6%)\n",
      "    ex_sport: 43,620 (3.7%)\n",
      "    com_purchase: 24,353 (2.1%)\n",
      "    ha_housework: 19,544 (1.7%)\n",
      "  Unique posture values: 6\n",
      "  Posture distribution:\n",
      "    mixed_move: 533,605 (45.3%)\n",
      "    sedentary: 492,471 (41.8%)\n",
      "    walk: 135,439 (11.5%)\n",
      "    running: 9,379 (0.8%)\n",
      "    biking: 6,096 (0.5%)\n",
      "    sport: 442 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "TEST SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total tests run: 50\n",
      "  ✓ Passed: 42\n",
      "  ✗ Failed: 3\n",
      "  ⚠ Warnings: 5\n",
      "\n",
      "❌ FAILED TESTS (3):\n",
      "  - Merge: 337 rows (3.21%) missing start_time\n",
      "  - Missing required columns: ['start_time_new']\n",
      "  - start_time_new: Column missing\n",
      "\n",
      "⚠ WARNINGS (5):\n",
      "  - Some (id, do) combinations in merge not in log_df: 2 unmatched\n",
      "  - start_time_new: 337 rows (3.21%) are null\n",
      "  - 103 rows classified as 'other'\n",
      "  - intensity: 743 nulls (0.06%) - within acceptable range\n",
      "  - 191210 duplicate rows found\n",
      "\n",
      "❌ SOME TESTS FAILED - REVIEW ABOVE\n",
      "   Data integrity may be compromised\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE DATA VALIDATION TESTS\n",
    "# ============================================================================\n",
    "# This cell performs extensive validation to ensure data integrity throughout\n",
    "# the processing pipeline. Run this after the main processing to verify:\n",
    "# - No data loss\n",
    "# - No unexpected NaN values\n",
    "# - Correct merges\n",
    "# - Data consistency\n",
    "# ============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE DATA VALIDATION TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = {\n",
    "    'passed': [],\n",
    "    'failed': [],\n",
    "    'warnings': []\n",
    "}\n",
    "\n",
    "def test_pass(test_name, message=\"\"):\n",
    "    \"\"\"Record a passed test\"\"\"\n",
    "    test_results['passed'].append(test_name)\n",
    "    print(f\"✓ PASS: {test_name}\")\n",
    "    if message:\n",
    "        print(f\"  → {message}\")\n",
    "\n",
    "def test_fail(test_name, message=\"\"):\n",
    "    \"\"\"Record a failed test\"\"\"\n",
    "    test_results['failed'].append(test_name)\n",
    "    print(f\"✗ FAIL: {test_name}\")\n",
    "    if message:\n",
    "        print(f\"  → {message}\")\n",
    "\n",
    "def test_warn(test_name, message=\"\"):\n",
    "    \"\"\"Record a warning\"\"\"\n",
    "    test_results['warnings'].append(test_name)\n",
    "    print(f\"⚠ WARN: {test_name}\")\n",
    "    if message:\n",
    "        print(f\"  → {message}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 1: Original Data Sources Integrity\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 1: Original Data Sources Integrity\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check original behav_am_df\n",
    "if 'behav_am_df' in locals():\n",
    "    orig_behav_rows = len(behav_am_df)\n",
    "    orig_behav_cols = len(behav_am_df.columns)\n",
    "    orig_behav_ids = behav_am_df['id'].nunique() if 'id' in behav_am_df.columns else 0\n",
    "    orig_behav_obs = behav_am_df['Observation'].nunique() if 'Observation' in behav_am_df.columns else 0\n",
    "    \n",
    "    test_pass(\"Original behav_am_df loaded\", \n",
    "              f\"Rows: {orig_behav_rows:,}, Columns: {orig_behav_cols}, IDs: {orig_behav_ids}, Observations: {orig_behav_obs}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['Observation', 'Behavior', 'Time_Relative_hms', 'Event_Type']\n",
    "    missing_cols = [c for c in required_cols if c not in behav_am_df.columns]\n",
    "    if missing_cols:\n",
    "        test_fail(\"Required columns missing in behav_am_df\", f\"Missing: {missing_cols}\")\n",
    "    else:\n",
    "        test_pass(\"All required columns present in behav_am_df\")\n",
    "else:\n",
    "    test_fail(\"behav_am_df not found in memory\")\n",
    "\n",
    "# Check original log_df\n",
    "if 'log_df' in locals():\n",
    "    orig_log_rows = len(log_df)\n",
    "    orig_log_ids = log_df['id'].nunique() if 'id' in log_df.columns else 0\n",
    "    orig_log_do = log_df['do'].nunique() if 'do' in log_df.columns else 0\n",
    "    \n",
    "    test_pass(\"Original log_df loaded\", \n",
    "              f\"Rows: {orig_log_rows}, IDs: {orig_log_ids}, DO values: {orig_log_do}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['id', 'do', 'start_time']\n",
    "    missing_cols = [c for c in required_cols if c not in log_df.columns]\n",
    "    if missing_cols:\n",
    "        test_fail(\"Required columns missing in log_df\", f\"Missing: {missing_cols}\")\n",
    "    else:\n",
    "        test_pass(\"All required columns present in log_df\")\n",
    "    \n",
    "    # Check for duplicate (id, do) combinations\n",
    "    if 'id' in log_df.columns and 'do' in log_df.columns:\n",
    "        duplicates = log_df.duplicated(subset=['id', 'do']).sum()\n",
    "        if duplicates > 0:\n",
    "            test_fail(f\"Duplicate (id, do) combinations in log_df\", f\"Found {duplicates} duplicates\")\n",
    "        else:\n",
    "            test_pass(\"No duplicate (id, do) combinations in log_df\")\n",
    "else:\n",
    "    test_fail(\"log_df not found in memory\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 2: ID and DO Extraction\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: ID and DO Extraction from Observation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_am_df' in locals() and 'Observation' in behav_am_df.columns:\n",
    "    # Check id extraction\n",
    "    if 'id' in behav_am_df.columns:\n",
    "        id_nulls = behav_am_df['id'].isna().sum()\n",
    "        id_unique = behav_am_df['id'].nunique()\n",
    "        if id_nulls == 0:\n",
    "            test_pass(f\"ID extraction: No nulls, {id_unique} unique IDs\")\n",
    "        else:\n",
    "            test_fail(f\"ID extraction: {id_nulls} null values found\")\n",
    "    else:\n",
    "        test_fail(\"ID column not found after extraction\")\n",
    "    \n",
    "    # Check do extraction\n",
    "    if 'do' in behav_am_df.columns:\n",
    "        do_nulls = behav_am_df['do'].isna().sum()\n",
    "        do_unique = behav_am_df['do'].nunique()\n",
    "        if do_nulls == 0:\n",
    "            test_pass(f\"DO extraction: No nulls, {do_unique} unique DO values\")\n",
    "        else:\n",
    "            test_fail(f\"DO extraction: {do_nulls} null values found\")\n",
    "        \n",
    "        # Verify extraction matches Observation pattern\n",
    "        sample_obs = behav_am_df['Observation'].head(10)\n",
    "        pattern_matches = 0\n",
    "        for obs in sample_obs:\n",
    "            if pd.notna(obs):\n",
    "                id_match = re.search(r'AM(\\d{2})', str(obs))\n",
    "                do_match = re.search(r'DO(\\d+)', str(obs))\n",
    "                if id_match and do_match:\n",
    "                    pattern_matches += 1\n",
    "        if pattern_matches == len(sample_obs.dropna()):\n",
    "            test_pass(\"Observation pattern extraction verified\")\n",
    "        else:\n",
    "            test_warn(\"Some Observation patterns may not match expected format\")\n",
    "    else:\n",
    "        test_fail(\"DO column not found after extraction\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 3: Merge with log_df\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 3: Merge with log_df\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_am_df3' in locals():\n",
    "    merge_rows = len(behav_am_df3)\n",
    "    merge_start_time_nulls = behav_am_df3['start_time'].isna().sum() if 'start_time' in behav_am_df3.columns else merge_rows\n",
    "    \n",
    "    # Check merge completeness\n",
    "    if merge_start_time_nulls == 0:\n",
    "        test_pass(f\"Merge complete: All {merge_rows:,} rows have start_time\")\n",
    "    else:\n",
    "        null_pct = (merge_start_time_nulls / merge_rows) * 100\n",
    "        if null_pct < 1:\n",
    "            test_warn(f\"Merge: {merge_start_time_nulls} rows ({null_pct:.2f}%) missing start_time\")\n",
    "        else:\n",
    "            test_fail(f\"Merge: {merge_start_time_nulls} rows ({null_pct:.2f}%) missing start_time\")\n",
    "    \n",
    "    # Check id and do after merge\n",
    "    if 'id' in behav_am_df3.columns:\n",
    "        id_nulls_after = behav_am_df3['id'].isna().sum()\n",
    "        if id_nulls_after == 0:\n",
    "            test_pass(\"ID preserved after merge: No nulls\")\n",
    "        else:\n",
    "            test_fail(f\"ID after merge: {id_nulls_after} null values\")\n",
    "    \n",
    "    if 'do' in behav_am_df3.columns:\n",
    "        do_nulls_after = behav_am_df3['do'].isna().sum()\n",
    "        if do_nulls_after == 0:\n",
    "            test_pass(\"DO preserved after merge: No nulls\")\n",
    "        else:\n",
    "            test_fail(f\"DO after merge: {do_nulls_after} null values\")\n",
    "        \n",
    "        # Check merge correctness: verify (id, do) combinations match log_df\n",
    "        if 'log_df' in locals() and 'id' in log_df.columns and 'do' in log_df.columns:\n",
    "            log_combos = set(zip(log_df['id'], log_df['do']))\n",
    "            merge_combos = set(zip(behav_am_df3['id'], behav_am_df3['do']))\n",
    "            unmatched = merge_combos - log_combos\n",
    "            if len(unmatched) == 0:\n",
    "                test_pass(\"All (id, do) combinations in merge match log_df\")\n",
    "            else:\n",
    "                test_warn(f\"Some (id, do) combinations in merge not in log_df: {len(unmatched)} unmatched\")\n",
    "    else:\n",
    "        test_fail(\"DO column missing after merge\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 4: start_time_new Computation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 4: start_time_new Computation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_am_df5' in locals():\n",
    "    if 'start_time_new' in behav_am_df5.columns:\n",
    "        stn_nulls = behav_am_df5['start_time_new'].isna().sum()\n",
    "        stn_total = len(behav_am_df5)\n",
    "        stn_pct = (stn_nulls / stn_total) * 100\n",
    "        \n",
    "        if stn_nulls == 0:\n",
    "            test_pass(f\"start_time_new: All {stn_total:,} rows have values\")\n",
    "        elif stn_pct < 5:\n",
    "            test_warn(f\"start_time_new: {stn_nulls} rows ({stn_pct:.2f}%) are null\")\n",
    "        else:\n",
    "            test_fail(f\"start_time_new: {stn_nulls} rows ({stn_pct:.2f}%) are null - CRITICAL ISSUE\")\n",
    "        \n",
    "        # Check format\n",
    "        if stn_nulls < stn_total:\n",
    "            sample_stn = behav_am_df5['start_time_new'].dropna().head(5)\n",
    "            valid_format = sample_stn.astype(str).str.match(r'\\d{1,2}:\\d{2}:\\d{2}\\s+(AM|PM)', case=False).all()\n",
    "            if valid_format:\n",
    "                test_pass(\"start_time_new format is correct (HH:MM:SS AM/PM)\")\n",
    "            else:\n",
    "                test_warn(\"start_time_new format may be incorrect\")\n",
    "    else:\n",
    "        test_fail(\"start_time_new column not found in behav_am_df5\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 5: Track Separation and Expansion\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 5: Track Separation and Expansion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'activity_df' in locals() and 'posture_df' in locals():\n",
    "    activity_rows = len(activity_df)\n",
    "    posture_rows = len(posture_df)\n",
    "    \n",
    "    test_pass(f\"Tracks separated: Activity={activity_rows:,}, Posture={posture_rows:,}\")\n",
    "    \n",
    "    # Check that all rows are classified\n",
    "    if 'df' in locals() and '_track' in df.columns:\n",
    "        other_count = (df['_track'] == 'other').sum()\n",
    "        if other_count == 0:\n",
    "            test_pass(\"All rows classified as activity or posture\")\n",
    "        else:\n",
    "            test_warn(f\"{other_count} rows classified as 'other'\")\n",
    "    \n",
    "    # Check expanded tracks\n",
    "    if 'activity_expanded' in locals() and 'posture_expanded' in locals():\n",
    "        act_exp_rows = len(activity_expanded)\n",
    "        pos_exp_rows = len(posture_expanded)\n",
    "        \n",
    "        test_pass(f\"Tracks expanded: Activity={act_exp_rows:,}, Posture={pos_exp_rows:,}\")\n",
    "        \n",
    "        # Check for required columns in expanded tracks\n",
    "        required_cols = ['Observation', '_second', 'id', 'do']\n",
    "        for track_name, track_df in [('activity_expanded', activity_expanded), \n",
    "                                      ('posture_expanded', posture_expanded)]:\n",
    "            missing = [c for c in required_cols if c not in track_df.columns]\n",
    "            if missing:\n",
    "                test_fail(f\"{track_name} missing columns: {missing}\")\n",
    "            else:\n",
    "                test_pass(f\"{track_name} has all required columns\")\n",
    "                \n",
    "            # Check for nulls in key columns\n",
    "            if 'id' in track_df.columns:\n",
    "                id_nulls = track_df['id'].isna().sum()\n",
    "                if id_nulls > 0:\n",
    "                    test_fail(f\"{track_name}: {id_nulls} null IDs\")\n",
    "                else:\n",
    "                    test_pass(f\"{track_name}: No null IDs\")\n",
    "            \n",
    "            if 'do' in track_df.columns:\n",
    "                do_nulls = track_df['do'].isna().sum()\n",
    "                if do_nulls > 0:\n",
    "                    test_fail(f\"{track_name}: {do_nulls} null DO values\")\n",
    "                else:\n",
    "                    test_pass(f\"{track_name}: No null DO values\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 6: Merged Result\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 6: Merged Activity and Posture Tracks\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'merged' in locals() or 'behav_am_df_6' in locals():\n",
    "    merged_df = merged if 'merged' in locals() else behav_am_df_6\n",
    "    merged_rows = len(merged_df)\n",
    "    \n",
    "    test_pass(f\"Merged result: {merged_rows:,} rows\")\n",
    "    \n",
    "    # Check key columns\n",
    "    key_cols = ['Observation', 'id', 'do', 'Time_Relative_hms_new']\n",
    "    for col in key_cols:\n",
    "        if col in merged_df.columns:\n",
    "            nulls = merged_df[col].isna().sum()\n",
    "            null_pct = (nulls / merged_rows) * 100\n",
    "            if nulls == 0:\n",
    "                test_pass(f\"{col}: No nulls\")\n",
    "            elif null_pct < 1:\n",
    "                test_warn(f\"{col}: {nulls} nulls ({null_pct:.2f}%)\")\n",
    "            else:\n",
    "                test_fail(f\"{col}: {nulls} nulls ({null_pct:.2f}%)\")\n",
    "        else:\n",
    "            test_fail(f\"{col}: Column missing\")\n",
    "    \n",
    "    # Check start_time_new in merged\n",
    "    if 'start_time_new' in merged_df.columns:\n",
    "        stn_nulls = merged_df['start_time_new'].isna().sum()\n",
    "        stn_pct = (stn_nulls / merged_rows) * 100\n",
    "        if stn_nulls == 0:\n",
    "            test_pass(f\"start_time_new in merged: No nulls\")\n",
    "        elif stn_pct < 5:\n",
    "            test_warn(f\"start_time_new in merged: {stn_nulls} nulls ({stn_pct:.2f}%)\")\n",
    "        else:\n",
    "            test_fail(f\"start_time_new in merged: {stn_nulls} nulls ({stn_pct:.2f}%) - CRITICAL\")\n",
    "    \n",
    "    # Check that each Observation has consistent id and do\n",
    "    if 'Observation' in merged_df.columns and 'id' in merged_df.columns and 'do' in merged_df.columns:\n",
    "        obs_groups = merged_df.groupby('Observation')[['id', 'do']].nunique()\n",
    "        inconsistent_obs = obs_groups[(obs_groups['id'] > 1) | (obs_groups['do'] > 1)]\n",
    "        if len(inconsistent_obs) == 0:\n",
    "            test_pass(\"Each Observation has consistent id and do values\")\n",
    "        else:\n",
    "            test_fail(f\"{len(inconsistent_obs)} Observations have inconsistent id/do values\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 7: Final Output (behav_copy)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 7: Final Output (behav_copy)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_copy' in locals():\n",
    "    final_rows = len(behav_copy)\n",
    "    final_cols = behav_copy.columns.tolist()\n",
    "    \n",
    "    test_pass(f\"Final output shape: {final_rows:,} rows, {len(final_cols)} columns\")\n",
    "    \n",
    "    # Required columns\n",
    "    required_final_cols = ['id', 'obs', 'rel_time', 'activity_type', 'posture_waves', 'intensity', 'start_time_new']\n",
    "    missing_final = [c for c in required_final_cols if c not in final_cols]\n",
    "    if missing_final:\n",
    "        test_fail(f\"Missing required columns: {missing_final}\")\n",
    "    else:\n",
    "        test_pass(\"All required columns present\")\n",
    "    \n",
    "    # Check for nulls in critical columns\n",
    "    critical_cols = {\n",
    "        'id': 0,      # Should have no nulls\n",
    "        'obs': 0,     # Should have no nulls\n",
    "        'rel_time': 0, # Should have no nulls\n",
    "        'activity_type': 5,  # Allow up to 5% nulls (may be filled later)\n",
    "        'posture_waves': 5,  # Allow up to 5% nulls\n",
    "        'intensity': 10,     # Allow up to 10% nulls\n",
    "        'start_time_new': 0  # Should have no nulls after fix\n",
    "    }\n",
    "    \n",
    "    for col, max_null_pct in critical_cols.items():\n",
    "        if col in behav_copy.columns:\n",
    "            nulls = behav_copy[col].isna().sum()\n",
    "            null_pct = (nulls / final_rows) * 100\n",
    "            \n",
    "            if nulls == 0:\n",
    "                test_pass(f\"{col}: No nulls\")\n",
    "            elif null_pct <= max_null_pct:\n",
    "                test_warn(f\"{col}: {nulls} nulls ({null_pct:.2f}%) - within acceptable range\")\n",
    "            else:\n",
    "                test_fail(f\"{col}: {nulls} nulls ({null_pct:.2f}%) - exceeds threshold of {max_null_pct}%\")\n",
    "        else:\n",
    "            test_fail(f\"{col}: Column missing\")\n",
    "    \n",
    "    # Check data types\n",
    "    if 'id' in behav_copy.columns:\n",
    "        if behav_copy['id'].dtype in [np.int64, np.float64]:\n",
    "            test_pass(\"id column has numeric type\")\n",
    "        else:\n",
    "            test_warn(f\"id column type: {behav_copy['id'].dtype}\")\n",
    "    \n",
    "    if 'obs' in behav_copy.columns:\n",
    "        if behav_copy['obs'].dtype in [np.int64, np.float64]:\n",
    "            test_pass(\"obs column has numeric type\")\n",
    "        else:\n",
    "            test_warn(f\"obs column type: {behav_copy['obs'].dtype}\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = behav_copy.duplicated().sum()\n",
    "    if duplicates == 0:\n",
    "        test_pass(\"No duplicate rows in final output\")\n",
    "    else:\n",
    "        test_warn(f\"{duplicates} duplicate rows found\")\n",
    "    \n",
    "    # Check id and obs ranges\n",
    "    if 'id' in behav_copy.columns and 'obs' in behav_copy.columns:\n",
    "        id_range = (behav_copy['id'].min(), behav_copy['id'].max())\n",
    "        obs_range = (behav_copy['obs'].min(), behav_copy['obs'].max())\n",
    "        test_pass(f\"id range: {id_range[0]:.0f} to {id_range[1]:.0f}, obs range: {obs_range[0]:.0f} to {obs_range[1]:.0f}\")\n",
    "        \n",
    "        # Check for unexpected id values (should match log_df)\n",
    "        if 'log_df' in locals() and 'id' in log_df.columns:\n",
    "            log_ids = set(log_df['id'].unique())\n",
    "            final_ids = set(behav_copy['id'].dropna().unique())\n",
    "            unexpected = final_ids - log_ids\n",
    "            if unexpected:\n",
    "                test_warn(f\"Unexpected id values in final output: {unexpected}\")\n",
    "            else:\n",
    "                test_pass(\"All id values match log_df\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 8: Data Loss Check\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 8: Data Loss Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare row counts at different stages\n",
    "if 'behav_am_df' in locals() and 'behav_copy' in locals():\n",
    "    # Count unique observations\n",
    "    if 'Observation' in behav_am_df.columns:\n",
    "        orig_obs_count = behav_am_df['Observation'].nunique()\n",
    "        \n",
    "        if 'behav_am_df_7' in locals() and 'Observation' in behav_am_df_7.columns:\n",
    "            final_obs_count = behav_am_df_7['Observation'].nunique()\n",
    "            if final_obs_count == orig_obs_count:\n",
    "                test_pass(f\"No observation loss: {orig_obs_count} observations preserved\")\n",
    "            elif final_obs_count > orig_obs_count:\n",
    "                test_warn(f\"Observation count increased: {orig_obs_count} → {final_obs_count} (expected due to expansion)\")\n",
    "            else:\n",
    "                loss_pct = ((orig_obs_count - final_obs_count) / orig_obs_count) * 100\n",
    "                if loss_pct < 1:\n",
    "                    test_warn(f\"Minor observation loss: {orig_obs_count} → {final_obs_count} ({loss_pct:.2f}% loss)\")\n",
    "                else:\n",
    "                    test_fail(f\"Significant observation loss: {orig_obs_count} → {final_obs_count} ({loss_pct:.2f}% loss)\")\n",
    "        \n",
    "        # Check that all original observations are represented\n",
    "        if 'behav_am_df_7' in locals() and 'Observation' in behav_am_df_7.columns:\n",
    "            orig_obs = set(behav_am_df['Observation'].unique())\n",
    "            final_obs = set(behav_am_df_7['Observation'].unique())\n",
    "            missing_obs = orig_obs - final_obs\n",
    "            if missing_obs:\n",
    "                test_fail(f\"{len(missing_obs)} observations missing from final output\")\n",
    "            else:\n",
    "                test_pass(\"All original observations present in final output\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 9: Consistency Checks\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 9: Data Consistency Checks\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_copy' in locals():\n",
    "    # Check that rel_time is properly formatted\n",
    "    if 'rel_time' in behav_copy.columns:\n",
    "        sample_rel = behav_copy['rel_time'].dropna().head(100)\n",
    "        if len(sample_rel) > 0:\n",
    "            # Check format: should be HH:MM:SS\n",
    "            valid_format = sample_rel.astype(str).str.match(r'\\d{2}:\\d{2}:\\d{2}').sum()\n",
    "            if valid_format == len(sample_rel):\n",
    "                test_pass(\"rel_time format is correct (HH:MM:SS)\")\n",
    "            else:\n",
    "                test_warn(f\"rel_time format: {valid_format}/{len(sample_rel)} rows have correct format\")\n",
    "    \n",
    "    # Check activity_type values\n",
    "    if 'activity_type' in behav_copy.columns:\n",
    "        valid_activities = ['sleep', 'pc_groom', 'pc_other', 'ha_housework', 'ha_food', 'ha_interior', \n",
    "                           'ha_exterior', 'ha_lawn', 'ha_pets', 'ha_other', 'care_children', 'care_adults',\n",
    "                           'work_general', 'work_screen', 'edu_class', 'edu_other', 'com_church', \n",
    "                           'com_volunteer', 'com_purchase', 'ha_eat', 'les_social', 'les_screen', \n",
    "                           'ex_sport', 'les_attend', 'trav_pass', 'trav_drive', 'trav_bike', \n",
    "                           'trav_walk', 'trav_other', 'non_codable']\n",
    "        sample_act = behav_copy['activity_type'].dropna().head(1000)\n",
    "        if len(sample_act) > 0:\n",
    "            invalid_act = sample_act[~sample_act.isin(valid_activities + [np.nan])]\n",
    "            if len(invalid_act) == 0:\n",
    "                test_pass(\"All activity_type values are valid\")\n",
    "            else:\n",
    "                test_warn(f\"Found {len(invalid_act)} invalid activity_type values: {invalid_act.unique()[:5]}\")\n",
    "    \n",
    "    # Check posture_waves values\n",
    "    if 'posture_waves' in behav_copy.columns:\n",
    "        valid_postures = ['sedentary', 'mixed_move', 'walk', 'running', 'biking', 'sport', 'not_coded']\n",
    "        sample_pos = behav_copy['posture_waves'].dropna().head(1000)\n",
    "        if len(sample_pos) > 0:\n",
    "            invalid_pos = sample_pos[~sample_pos.isin(valid_postures + [np.nan])]\n",
    "            if len(invalid_pos) == 0:\n",
    "                test_pass(\"All posture_waves values are valid\")\n",
    "            else:\n",
    "                test_warn(f\"Found {len(invalid_pos)} invalid posture_waves values: {invalid_pos.unique()[:5]}\")\n",
    "    \n",
    "    # Check intensity values\n",
    "    if 'intensity' in behav_copy.columns:\n",
    "        valid_intensities = ['sedentary', 'light', 'moderate', 'vigorous', 'mvpa']\n",
    "        sample_int = behav_copy['intensity'].dropna().head(1000)\n",
    "        if len(sample_int) > 0:\n",
    "            invalid_int = sample_int[~sample_int.isin(valid_intensities + [np.nan])]\n",
    "            if len(invalid_int) == 0:\n",
    "                test_pass(\"All intensity values are valid\")\n",
    "            else:\n",
    "                test_warn(f\"Found {len(invalid_int)} invalid intensity values: {invalid_int.unique()[:5]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 10: Summary Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 10: Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'behav_copy' in locals():\n",
    "    print(f\"\\nFinal Output Summary:\")\n",
    "    print(f\"  Total rows: {len(behav_copy):,}\")\n",
    "    print(f\"  Total columns: {len(behav_copy.columns)}\")\n",
    "    \n",
    "    if 'id' in behav_copy.columns:\n",
    "        print(f\"  Unique IDs: {behav_copy['id'].nunique()}\")\n",
    "    if 'obs' in behav_copy.columns:\n",
    "        print(f\"  Unique observations: {behav_copy['obs'].nunique()}\")\n",
    "    if 'activity_type' in behav_copy.columns:\n",
    "        print(f\"  Unique activity types: {behav_copy['activity_type'].nunique()}\")\n",
    "        print(f\"  Activity type distribution:\")\n",
    "        act_dist = behav_copy['activity_type'].value_counts().head(10)\n",
    "        for act, count in act_dist.items():\n",
    "            print(f\"    {act}: {count:,} ({count/len(behav_copy)*100:.1f}%)\")\n",
    "    if 'posture_waves' in behav_copy.columns:\n",
    "        print(f\"  Unique posture values: {behav_copy['posture_waves'].nunique()}\")\n",
    "        print(f\"  Posture distribution:\")\n",
    "        pos_dist = behav_copy['posture_waves'].value_counts()\n",
    "        for pos, count in pos_dist.items():\n",
    "            print(f\"    {pos}: {count:,} ({count/len(behav_copy)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL TEST SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_tests = len(test_results['passed']) + len(test_results['failed']) + len(test_results['warnings'])\n",
    "print(f\"\\nTotal tests run: {total_tests}\")\n",
    "print(f\"  ✓ Passed: {len(test_results['passed'])}\")\n",
    "print(f\"  ✗ Failed: {len(test_results['failed'])}\")\n",
    "print(f\"  ⚠ Warnings: {len(test_results['warnings'])}\")\n",
    "\n",
    "if test_results['failed']:\n",
    "    print(f\"\\n❌ FAILED TESTS ({len(test_results['failed'])}):\")\n",
    "    for test in test_results['failed']:\n",
    "        print(f\"  - {test}\")\n",
    "\n",
    "if test_results['warnings']:\n",
    "    print(f\"\\n⚠ WARNINGS ({len(test_results['warnings'])}):\")\n",
    "    for test in test_results['warnings']:\n",
    "        print(f\"  - {test}\")\n",
    "\n",
    "if len(test_results['failed']) == 0:\n",
    "    print(\"\\n✅ ALL CRITICAL TESTS PASSED!\")\n",
    "    if len(test_results['warnings']) > 0:\n",
    "        print(\"   (Some warnings present - review above)\")\n",
    "else:\n",
    "    print(\"\\n❌ SOME TESTS FAILED - REVIEW ABOVE\")\n",
    "    print(\"   Data integrity may be compromised\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fba21c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>date_time</th>\n",
       "      <th>rel_time</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>posture_waves</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500000</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:45</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500001</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:46</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500002</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:47</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500003</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:48</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500004</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:49</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500005</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:50</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500006</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:51</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500007</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:52</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500008</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:53</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500009</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9/2/2017</td>\n",
       "      <td>01:14:51 PM</td>\n",
       "      <td>9/2/2017 01:14:51 PM</td>\n",
       "      <td>08:12:54</td>\n",
       "      <td>les_social</td>\n",
       "      <td>mixed_move</td>\n",
       "      <td>sedentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  obs      date         time             date_time  rel_time  \\\n",
       "500000  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:45   \n",
       "500001  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:46   \n",
       "500002  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:47   \n",
       "500003  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:48   \n",
       "500004  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:49   \n",
       "500005  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:50   \n",
       "500006  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:51   \n",
       "500007  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:52   \n",
       "500008  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:53   \n",
       "500009  11.0  2.0  9/2/2017  01:14:51 PM  9/2/2017 01:14:51 PM  08:12:54   \n",
       "\n",
       "       activity_type posture_waves  intensity  \n",
       "500000    les_social    mixed_move  sedentary  \n",
       "500001    les_social    mixed_move  sedentary  \n",
       "500002    les_social    mixed_move  sedentary  \n",
       "500003    les_social    mixed_move  sedentary  \n",
       "500004    les_social    mixed_move  sedentary  \n",
       "500005    les_social    mixed_move  sedentary  \n",
       "500006    les_social    mixed_move  sedentary  \n",
       "500007    les_social    mixed_move  sedentary  \n",
       "500008    les_social    mixed_move  sedentary  \n",
       "500009    les_social    mixed_move  sedentary  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_copy.iloc[500000:500010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dfe71cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>type</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>session</th>\n",
       "      <th>do</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DO2</td>\n",
       "      <td>L</td>\n",
       "      <td>18:44:45</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>2:02:15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/3/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>16:43:57</td>\n",
       "      <td>18:45:00</td>\n",
       "      <td>2:01:03</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10/6/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DO1</td>\n",
       "      <td>H</td>\n",
       "      <td>13:17:10</td>\n",
       "      <td>15:17:32</td>\n",
       "      <td>2:00:22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7/24/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_a</td>\n",
       "      <td>A</td>\n",
       "      <td>8:00:27</td>\n",
       "      <td>8:52:32</td>\n",
       "      <td>0:52:05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7/25/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>DO2_b</td>\n",
       "      <td>A</td>\n",
       "      <td>8:56:13</td>\n",
       "      <td>10:12:29</td>\n",
       "      <td>1:16:16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7/25/2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    obs type start_time stop_time duration  session  do       date\n",
       "0   1    DO2    L   18:44:45  20:47:00  2:02:15        1   1  10/3/2017\n",
       "1   1    DO1    H   16:43:57  18:45:00  2:01:03        2   2  10/6/2017\n",
       "2   2    DO1    H   13:17:10  15:17:32  2:00:22        1   1  7/24/2017\n",
       "3   2  DO2_a    A    8:00:27   8:52:32  0:52:05        2   2  7/25/2017\n",
       "4   2  DO2_b    A    8:56:13  10:12:29  1:16:16        3   3  7/25/2017"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
